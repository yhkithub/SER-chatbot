import streamlit as st
import torch
import torchaudio
import os
<<<<<<< HEAD
from transformers import AutoModelForAudioClassification, AutoProcessor
import torchaudio.transforms as T
=======
from datetime import datetime
import time
from transformers import AutoModelForAudioClassification, AutoProcessor
import torchaudio.transforms as T

>>>>>>> 7b5f7e1 (feat: ëŒ€í™” í†µê³„ ê¸°ëŠ¥ ì¶”ê°€ ë° í˜ë¥´ì†Œë‚˜ ì „í™˜ ë²„ê·¸ ìˆ˜ì •)
from src.core.services.chatbot_service import ChatbotService
from src.app.config import OpenAIConfig
from src.utils.audio_handler import process_audio_file
from src.components.message_display import apply_chat_styles, display_message, get_emotion_color
from src.core.services.personas import PERSONAS
<<<<<<< HEAD

# ìŒì„± ê°ì • ì¸ì‹ ëª¨ë¸ ì„¤ì •
model_name = "forwarder1121/ast-finetuned-model"
processor = AutoProcessor.from_pretrained(model_name)
model = AutoModelForAudioClassification.from_pretrained(model_name)

# ê°ì • ë§¤í•‘
EMOTION_MAPPING = {
    0: "Anger",
    1: "Disgust", 
    2: "Fear",
    3: "Happy",
    4: "Neutral",
    5: "Sad"
}


def get_emotion_from_gpt(prompt: str) -> str:
    """
    GPTë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ê°ì •ì„ ì¶”ë¡ í•˜ê³  í‘œì¤€í™”ëœ ê°’ ë°˜í™˜.
    """
    predefined_emotions = ["Anger", "Disgust", "Fear", "Happy", "Neutral", "Sad"]
    emotion_prompt = (
        f"The user said: \"{prompt}\".\n"
        f"Classify the user's input into one of these emotions: {', '.join(predefined_emotions)}.\n"
        f"Respond ONLY with the emotion name (e.g., Happy, Neutral).\n"
    )

    # OpenAI API í˜¸ì¶œ
    response = st.session_state.chatbot_service.llm.invoke(emotion_prompt)
    detected_emotion = response.content.strip()  # ì‘ë‹µì—ì„œ ê°ì • ì¶”ì¶œ
    print(f"[DEBUG] Detected Emotion: {detected_emotion}")

    if detected_emotion not in predefined_emotions:
        print(f"[DEBUG] Unexpected emotion: {detected_emotion}")
        detected_emotion = "Neutral"  # ê¸°ë³¸ê°’ ì„¤ì •

    return detected_emotion


def process_audio(waveform, target_sample_rate=16000, target_length=16000):
    """Process audio to correct format."""
    try:
        if waveform.shape[0] > 1:  # ë‹¤ì±„ë„ ì˜¤ë””ì˜¤ì¸ ê²½ìš° í‰ê·  ì²˜ë¦¬
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        if waveform.shape[1] > 0:
            current_sample_rate = target_sample_rate
            if current_sample_rate != target_sample_rate:
                resampler = T.Resample(orig_freq=current_sample_rate, new_freq=target_sample_rate)
                waveform = resampler(waveform)

        if waveform.shape[1] < target_length:
            padding_length = target_length - waveform.shape[1]
            waveform = torch.nn.functional.pad(waveform, (0, padding_length))
        else:
            start = (waveform.shape[1] - target_length) // 2
            waveform = waveform[:, start:start + target_length]

        return waveform
    except Exception as e:
        st.error(f"Error in audio processing: {str(e)}")
        return None


def predict_audio_emotion(audio_path):
    """Predict emotion from audio file."""
    try:
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        waveform, sample_rate = torchaudio.load(audio_path)
        print(f"[DEBUG] ì˜¤ë””ì˜¤ ë¡œë“œ ì™„ë£Œ: Waveform Shape: {waveform.shape}, Sample Rate: {sample_rate}")

        # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
        processed_waveform = process_audio(waveform, target_sample_rate=16000)
        if processed_waveform is None:
            print("[ERROR] ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            return None
        print(f"[DEBUG] ì „ì²˜ë¦¬ëœ Waveform Shape: {processed_waveform.shape}")

        # ëª¨ë¸ ì…ë ¥ ìƒì„±
        inputs = processor(processed_waveform.squeeze(), sampling_rate=16000, return_tensors="pt")
        print(f"[DEBUG] ëª¨ë¸ ì…ë ¥ ìƒì„± ì™„ë£Œ: {inputs.keys()}")

        # ëª¨ë¸ ì˜ˆì¸¡
        with torch.no_grad():
            outputs = model(**inputs)
        print(f"[DEBUG] ëª¨ë¸ ì¶œë ¥: {outputs.logits}")

        # ì˜ˆì¸¡ëœ ê°ì • ì¸ë±ìŠ¤
        predicted_class_idx = outputs.logits.argmax(-1).item()
        print(f"[DEBUG] ê°ì • ë¶„ì„ ê²°ê³¼ Index: {predicted_class_idx}")

        # ê°ì • ë§¤í•‘
        emotion = EMOTION_MAPPING.get(predicted_class_idx, "Unknown")
        print(f"[DEBUG] ê°ì • ë¶„ì„ ê²°ê³¼ Emotion: {emotion}")
        return emotion

    except Exception as e:
        print(f"[ERROR] ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def handle_audio_upload(uploaded_audio):
    """
    ìŒì„± íŒŒì¼ ì—…ë¡œë“œ í•¸ë“¤ëŸ¬
    """
    temp_audio_path = "temp_audio.wav"
    try:
        with open(temp_audio_path, "wb") as f:
            f.write(uploaded_audio.getbuffer())

        print(f"[DEBUG] ì„ì‹œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {temp_audio_path}")

        # ìŒì„± -> í…ìŠ¤íŠ¸ ë³€í™˜
        with st.spinner("ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘..."):
            audio_text = process_audio_file(uploaded_audio.read(), temp_audio_path)
            if not audio_text:
                st.warning("ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

        # ê°ì • ë¶„ì„
        with st.spinner("ê°ì • ë¶„ì„ ì¤‘..."):
            audio_emotion = predict_audio_emotion(temp_audio_path)
            if not audio_emotion:
                st.warning("ìŒì„± ê°ì •ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

        # ê°ì • ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.current_emotion = audio_emotion
        update_conversation_stats(audio_emotion)  # ëŒ€í™” í†µê³„ ì—…ë°ì´íŠ¸

        # ì„ íƒëœ í˜ë¥´ì†Œë‚˜ ê°€ì ¸ì˜¤ê¸°
        persona_name = st.session_state.get("selected_persona", "ê¹€ì†Œì—° ì„ ìƒë‹˜")

        # GPT ì‘ë‹µ ìƒì„±
        with st.spinner("GPT ì‘ë‹µ ìƒì„± ì¤‘..."):
            gpt_prompt = (
                f"The user uploaded an audio file. Here is the transcribed text: '{audio_text}'.\n"
                f"The detected emotion is '{audio_emotion}'.\n"
                f"Respond to the user in the selected persona: {persona_name}."
            )
            chatbot = st.session_state.chatbot_service
            gpt_response = chatbot.get_response(gpt_prompt, persona_name)

        # ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
        current_time = datetime.now().strftime('%p %I:%M')
        st.session_state.messages.append({
            "role": "user",
            "content": f"[ìŒì„± íŒŒì¼] í…ìŠ¤íŠ¸: {audio_text}",
            "emotion": audio_emotion,
            "timestamp": current_time
        })
        st.session_state.messages.append({
            "role": "assistant",
            "content": gpt_response,
            "timestamp": current_time
        })

        print("[DEBUG] ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

    except Exception as e:
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"[ERROR] handle_audio_uploadì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")

    finally:
        if os.path.exists(temp_audio_path):
            os.remove(temp_audio_path)
            print(f"[DEBUG] ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {temp_audio_path}")

        # UI ê°•ì œ ê°±ì‹ 
        st.rerun()


# def handle_audio_upload(uploaded_audio):
#     """
#     ìŒì„± íŒŒì¼ ì—…ë¡œë“œ í•¸ë“¤ëŸ¬
#     """
#     temp_audio_path = "temp_audio.wav"
#     try:
#         # ì„ì‹œ íŒŒì¼ ì €ì¥
#         with open(temp_audio_path, "wb") as f:
#             f.write(uploaded_audio.getbuffer())

#         print(f"[DEBUG] ì„ì‹œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {temp_audio_path}")

#         # í…ìŠ¤íŠ¸ ë³€í™˜
#         with st.spinner("í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘..."):
#             audio_text = process_audio_file(uploaded_audio.read(), temp_audio_path)
#             if not audio_text:
#                 st.warning("ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#                 return

#         # ê°ì • ë¶„ì„
#         with st.spinner("ê°ì • ë¶„ì„ ì¤‘..."):
#             audio_emotion = predict_audio_emotion(temp_audio_path)
#             if not audio_emotion:
#                 st.warning("ìŒì„± ê°ì •ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#                 return

#         # ê²°ê³¼ ì—…ë°ì´íŠ¸
#         current_time = datetime.now().strftime('%p %I:%M')
#         st.session_state.messages.append({
#             "role": "user",
#             "content": f"[ìŒì„± íŒŒì¼] í…ìŠ¤íŠ¸: {audio_text}",
#             "emotion": audio_emotion,
#             "timestamp": current_time
#         })
#         st.session_state.messages.append({
#             "role": "assistant",
#             "content": f"ìŒì„±ì—ì„œ '{audio_text}'ë¥¼ ê°ì§€í–ˆìœ¼ë©°, ê°ì •ì€ '{audio_emotion}'ì…ë‹ˆë‹¤.",
#             "timestamp": current_time
#         })

#         update_conversation_stats(audio_emotion)
#         print("[DEBUG] ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

#     except Exception as e:
#         st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
#         print(f"[ERROR] handle_audio_uploadì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")

#     finally:
#         # ëª¨ë“  ì‘ì—…ì´ ëë‚œ í›„ íŒŒì¼ ì‚­ì œ
#         if os.path.exists(temp_audio_path):
#             os.remove(temp_audio_path)
#             print(f"[DEBUG] ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {temp_audio_path}")



def update_conversation_stats(emotion: str):
    """Update conversation statistics based on the detected emotion."""
    st.session_state.conversation_stats['total'] += 1
    if emotion in ['Happy', 'Neutral']:
        st.session_state.conversation_stats['positive'] += 1
    elif emotion in ['Anger', 'Disgust', 'Fear', 'Sad']:
        st.session_state.conversation_stats['negative'] += 1

def main():
    st.set_page_config(
        page_title="ê°ì •ì¸ì‹ ì±—ë´‡",
        page_icon="ğŸ¤—",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # ìƒíƒœ ì´ˆê¸°í™”
    if 'initialized' not in st.session_state:
        st.session_state.initialized = True
        st.session_state.chatbot_service = ChatbotService(OpenAIConfig())
        st.session_state.messages = [{
            'role': 'assistant',
            'content': "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë– ì…¨ë‚˜ìš”? ê¸°ë¶„ì´ë‚˜ ê°ì •ì„ ììœ ë¡­ê²Œ ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”. í…ìŠ¤íŠ¸ë¡œ ì…ë ¥í•˜ê±°ë‚˜ ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”. ğŸ˜Š",
            'timestamp': datetime.now().strftime('%p %I:%M')
        }]
        st.session_state.last_uploaded_audio = None
        st.session_state.current_emotion = "Neutral"
=======
from src.utils.state_management import (
    initialize_session_state, 
    clear_session_state, 
    ensure_state_initialization
)
from src.components.chat_components import (
    render_emotion_indicator,
    render_conversation_stats
)
from src.app.constants import (
    DEFAULT_PERSONA, 
    DEFAULT_EMOTION, 
    EMOTIONS,
    EMOTION_MAPPING
)

# ìŒì„± ê°ì • ì¸ì‹ ëª¨ë¸ ì„¤ì •
MODEL_NAME = "forwarder1121/ast-finetuned-model"
processor = AutoProcessor.from_pretrained(MODEL_NAME)
model = AutoModelForAudioClassification.from_pretrained(MODEL_NAME)

def get_emotion_from_gpt(prompt: str) -> str:
    """
    GPTë¥¼ í†µí•´ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        prompt (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
        
    Returns:
        str: ê°ì§€ëœ ê°ì • (Anger, Disgust, Fear, Happy, Neutral, Sad ì¤‘ í•˜ë‚˜)
    """
    predefined_emotions = list(EMOTION_MAPPING.values())
    emotion_prompt = (
        f"The user said: \"{prompt}\".\n"
        f"Classify the user's input into one of these emotions: {', '.join(predefined_emotions)}.\n"
        f"Respond ONLY with the emotion name (e.g., Happy, Neutral).\n"
    )

    response = st.session_state.chatbot_service.llm.invoke(emotion_prompt)
    detected_emotion = response.content.strip()

    if detected_emotion not in predefined_emotions:
        detected_emotion = DEFAULT_EMOTION

    return detected_emotion

def process_audio(waveform: torch.Tensor, target_sample_rate: int = 16000, target_length: int = 16000) -> torch.Tensor:
    """
    ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        waveform (torch.Tensor): ì…ë ¥ ì˜¤ë””ì˜¤ ë°ì´í„°
        target_sample_rate (int): ëª©í‘œ ìƒ˜í”Œë§ ë ˆì´íŠ¸
        target_length (int): ëª©í‘œ ê¸¸ì´
        
    Returns:
        torch.Tensor: ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
    """
    try:
        # ë‹¤ì±„ë„ ì˜¤ë””ì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë³€í™˜
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        # ìƒ˜í”Œë§ ë ˆì´íŠ¸ ì¡°ì •
        if waveform.shape[1] > 0:
            current_sample_rate = target_sample_rate
            if current_sample_rate != target_sample_rate:
                resampler = T.Resample(orig_freq=current_sample_rate, new_freq=target_sample_rate)
                waveform = resampler(waveform)

        # ê¸¸ì´ ì¡°ì •
        if waveform.shape[1] < target_length:
            padding_length = target_length - waveform.shape[1]
            waveform = torch.nn.functional.pad(waveform, (0, padding_length))
        else:
            start = (waveform.shape[1] - target_length) // 2
            waveform = waveform[:, start:start + target_length]

        return waveform
    except Exception as e:
        st.error(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def predict_audio_emotion(audio_path: str) -> str:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ê°ì •ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    
    Args:
        audio_path (str): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        str: ì˜ˆì¸¡ëœ ê°ì •
    """
    try:
        # ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì „ì²˜ë¦¬
        waveform, sample_rate = torchaudio.load(audio_path)
        processed_waveform = process_audio(waveform)
        if processed_waveform is None:
            return None

        # ëª¨ë¸ ì…ë ¥ ìƒì„± ë° ì˜ˆì¸¡
        inputs = processor(processed_waveform.squeeze(), sampling_rate=16000, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)

        # ì˜ˆì¸¡ ê²°ê³¼ ë³€í™˜
        predicted_class_idx = outputs.logits.argmax(-1).item()
        return EMOTION_MAPPING.get(predicted_class_idx, DEFAULT_EMOTION)

    except Exception as e:
        st.error(f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def handle_chat_message(prompt: str, current_persona: str) -> tuple:
    """
    ì±„íŒ… ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ê°ì • ë¶„ì„
    user_emotion = get_emotion_from_gpt(prompt)
    st.session_state.current_emotion = user_emotion
    
    # GPT ì‘ë‹µ ìƒì„±
    response = st.session_state.chatbot_service.get_response(prompt, current_persona)
    
    return user_emotion, response

def add_chat_message(role: str, content: str, emotion: str = None):
    """
    ì±„íŒ… ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    current_time = datetime.now().strftime('%p %I:%M')
    message = {
        "role": role,
        "content": content,
        "timestamp": current_time
    }
    if emotion:
        message["emotion"] = emotion
    
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    st.session_state.messages.append(message)

def render_chat_area():
    """ì±„íŒ… ì˜ì—­ì„ ë Œë”ë§í•©ë‹ˆë‹¤."""
    st.title("ì±„íŒ…")

    # ë©”ì‹œì§€ í‘œì‹œ
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            display_message(message, persona=st.session_state.selected_persona)

    # ì±„íŒ… ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
        if prompt.strip():
            try:
                # í˜„ì¬ ìƒíƒœ ì €ì¥
                current_persona = st.session_state.selected_persona
                
                # ì±—ë´‡ ì„œë¹„ìŠ¤ í™•ì¸
                if 'chatbot_service' not in st.session_state:
                    initialize_session_state(current_persona)
                
                # ë©”ì‹œì§€ ì²˜ë¦¬
                user_emotion, response = handle_chat_message(prompt, current_persona)
                
                # ëŒ€í™” í†µê³„ ì—…ë°ì´íŠ¸
                update_conversation_stats(user_emotion)
                
                # ë©”ì‹œì§€ ì¶”ê°€
                add_chat_message("user", prompt, user_emotion)
                add_chat_message("assistant", response)
                
                # ì„¸ì…˜ ìƒíƒœì— ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                st.session_state.processed = True
                
            except Exception as e:
                st.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    # ë©”ì‹œì§€ ì²˜ë¦¬ í›„ í™”ë©´ ê°±ì‹ 
    if st.session_state.get('processed', False):
        st.session_state.processed = False
        st.rerun()

def render_chat_page():
    """ì±„íŒ… í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    selected_persona = st.query_params.get("persona")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”ê°€ í•„ìš”í•œ ê²½ìš°
    if ('initialized' not in st.session_state or 
        'selected_persona' not in st.session_state or 
        st.session_state.selected_persona != selected_persona):
        clear_session_state()
        initialize_session_state(selected_persona)
    
    render_sidebar()
    render_chat_area()

def render_sidebar():
    """ì‚¬ì´ë“œë°”ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    with st.sidebar:
        st.title("ê°ì •ì¸ì‹ ì±—ë´‡ ğŸ ")
        
        # í™ˆìœ¼ë¡œ ëŒì•„ê°€ê¸° ë²„íŠ¼
        if st.button("â† ë‹¤ë¥¸ í˜ë¥´ì†Œë‚˜ ì„ íƒí•˜ê¸°", key="change_persona_button"):
            # ì„¸ì…˜ ìƒíƒœ ì™„ì „ ì´ˆê¸°í™”
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            
            # URL íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
            st.query_params.clear()
            st.query_params["page"] = "home"  # í™ˆ í˜ì´ì§€ë¡œ ì´ë™
            st.rerun()

        st.markdown("### ì‚¬ìš© ë°©ë²•")
        st.markdown("""
        1. ì±„íŒ…ì°½ì— í˜„ì¬ ê¸°ë¶„ì´ë‚˜ ìƒí™©ì„ ì…ë ¥í•˜ì„¸ìš”.
        2. ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ê°ì •ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        3. ì±—ë´‡ì´ ê°ì •ì„ ë¶„ì„í•˜ê³  ê³µê°ì ì¸ ëŒ€í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        4. í•„ìš”í•œ ê²½ìš° ì ì ˆí•œ ì¡°ì–¸ì´ë‚˜ ìœ„ë¡œë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """)

        # í˜„ì¬ í˜ë¥´ì†Œë‚˜ í‘œì‹œ
        current_persona = st.session_state.get('selected_persona', st.query_params.get("persona"))
        st.markdown(f"### í˜„ì¬ ëŒ€í™” ìƒëŒ€: {current_persona}")

        # ìƒíƒœ ì´ˆê¸°í™” ë° í‘œì‹œ
        ensure_state_initialization('current_emotion', DEFAULT_EMOTION)
        ensure_state_initialization('conversation_stats', {'total': 0, 'positive': 0, 'negative': 0})
        render_emotion_indicator(st.session_state.current_emotion)
        render_conversation_stats(st.session_state.conversation_stats)

        # ìŒì„± íŒŒì¼ ì—…ë¡œë“œ
        st.markdown("### ìŒì„± íŒŒì¼ ì—…ë¡œë“œ")
        uploaded_audio = st.file_uploader("ì§€ì› í˜•ì‹: WAV", type=["wav"])
        if uploaded_audio is not None and uploaded_audio != st.session_state.get('last_uploaded_audio'):
            st.session_state.last_uploaded_audio = uploaded_audio
            handle_audio_upload(uploaded_audio)

def update_conversation_stats(emotion: str):
    """
    ëŒ€í™” í†µê³„ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    
    Args:
        emotion (str): ê°ì§€ëœ ê°ì •
    """
    if 'conversation_stats' not in st.session_state:
>>>>>>> 7b5f7e1 (feat: ëŒ€í™” í†µê³„ ê¸°ëŠ¥ ì¶”ê°€ ë° í˜ë¥´ì†Œë‚˜ ì „í™˜ ë²„ê·¸ ìˆ˜ì •)
        st.session_state.conversation_stats = {
            'total': 0,
            'positive': 0,
            'negative': 0
        }
<<<<<<< HEAD
        st.session_state.selected_persona = "ê¹€ì†Œì—° ì„ ìƒë‹˜"  # ê¸°ë³¸ í˜ë¥´ì†Œë‚˜

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.title("ê°ì •ì¸ì‹ ì±—ë´‡ ğŸ ")

        st.markdown("### ì‚¬ìš© ë°©ë²•")
        st.markdown("""
        1. ì±„íŒ…ì°½ì— í˜„ì¬ ê¸°ë¶„ì´ë‚˜ ìƒí™©ì„ ì…ë ¥í•˜ì„¸ìš”.
        2. ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ê°ì •ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        3. ì±—ë´‡ì´ ê°ì •ì„ ë¶„ì„í•˜ê³  ê³µê°ì ì¸ ëŒ€í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        4. í•„ìš”í•œ ê²½ìš° ì ì ˆí•œ ì¡°ì–¸ì´ë‚˜ ìœ„ë¡œë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """)

        st.markdown("### í˜ë¥´ì†Œë‚˜ ì„ íƒ")
        selected_persona = st.selectbox("í˜ë¥´ì†Œë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:", list(PERSONAS.keys()))
        st.session_state.selected_persona = selected_persona

        # í˜„ì¬ ê°ì • ìƒíƒœ í‘œì‹œ
        st.markdown("### í˜„ì¬ ê°ì • ìƒíƒœ")
        emotion = st.session_state.current_emotion
        emotion_color = get_emotion_color(emotion)
        st.markdown(f"""
        <div style="
            display: flex;
            align-items: center;
            gap: 8px;
            margin-top: 16px;
        ">
            <span style="
                background-color: {emotion_color};
                color: white;
                padding: 4px 12px;
                border-radius: 12px;
                font-weight: 600;
            ">{emotion}</span>
        </div>
        """, unsafe_allow_html=True)

        # ëŒ€í™” í†µê³„
        stats = st.session_state.conversation_stats
        st.markdown("### ëŒ€í™” í†µê³„")
        st.write(f"- ì´ ëŒ€í™” ìˆ˜: {stats.get('total', 0)}")
        st.write(f"- ê¸ì •ì  ê°ì •: {stats.get('positive', 0)}")
        st.write(f"- ë¶€ì •ì  ê°ì •: {stats.get('negative', 0)}")

        # ìŒì„± íŒŒì¼ ì—…ë¡œë“œ
        st.markdown("### ìŒì„± íŒŒì¼ ì—…ë¡œë“œ")
        uploaded_audio = st.file_uploader("ì§€ì› í˜•ì‹: WAV", type=["wav"])
        if uploaded_audio is not None and uploaded_audio != st.session_state.last_uploaded_audio:
            st.session_state.last_uploaded_audio = uploaded_audio
            handle_audio_upload(uploaded_audio)

    # ë©”ì¸ ì±„íŒ… ì˜ì—­
    st.title("ì±„íŒ…")

    # ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.get('messages', []):
        with st.chat_message(message["role"]):
            display_message(message, persona=selected_persona)

    # í…ìŠ¤íŠ¸ ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
        if prompt.strip():
            chatbot = st.session_state.chatbot_service
            persona_name = st.session_state.get("selected_persona", "ê¹€ì†Œì—° ì„ ìƒë‹˜")
    
            # ê°ì • ë¶„ì„
            user_emotion = get_emotion_from_gpt(prompt)
            st.session_state.current_emotion = user_emotion  # í˜„ì¬ ê°ì • ìƒíƒœ ì—…ë°ì´íŠ¸
    
            # ëŒ€í™” í†µê³„ ì—…ë°ì´íŠ¸
            update_conversation_stats(user_emotion)
    
            # GPT ì‘ë‹µ ìƒì„±
            response = chatbot.get_response(prompt, persona_name)
    
            # ë©”ì‹œì§€ ì €ì¥
            current_time = datetime.now().strftime('%p %I:%M')
            st.session_state.messages.append({
                "role": "user",
                "content": prompt,
                "emotion": user_emotion,
                "timestamp": current_time
            })
            st.session_state.messages.append({
                "role": "assistant",
                "content": response,
                "timestamp": current_time
            })
    
            # í™”ë©´ ê°±ì‹ 
=======
    
    # ì „ì²´ ëŒ€í™” ìˆ˜ ì¦ê°€
    st.session_state.conversation_stats['total'] += 1
    
    # ê°ì •ì— ë”°ë¥¸ í†µê³„ ì—…ë°ì´íŠ¸
    positive_emotions = ['Happy']
    negative_emotions = ['Anger', 'Disgust', 'Fear', 'Sad']
    
    if emotion in positive_emotions:
        st.session_state.conversation_stats['positive'] += 1
    elif emotion in negative_emotions:
        st.session_state.conversation_stats['negative'] += 1

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="ê°ì •ì¸ì‹ ì±—ë´‡",
        page_icon="ğŸ¤—",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # í˜ì¬ í˜ì´ì§€ í™•ì¸
    current_page = st.query_params.get("page", "home")
    
    # í˜ì´ì§€ ë¼ìš°íŒ…
    if current_page == "chat":
        # í˜ë¥´ì†Œë‚˜ í™•ì¸
        selected_persona = st.query_params.get("persona")
        if not selected_persona:
            st.query_params.clear()
            st.query_params["page"] = "home"
>>>>>>> 7b5f7e1 (feat: ëŒ€í™” í†µê³„ ê¸°ëŠ¥ ì¶”ê°€ ë° í˜ë¥´ì†Œë‚˜ ì „í™˜ ë²„ê·¸ ìˆ˜ì •)
            st.rerun()
        else:
            # URL íŒŒë¼ë¯¸í„° ìœ ì§€
            st.query_params["page"] = "chat"
            st.query_params["persona"] = selected_persona
            render_chat_page()
    else:
        from src.app.home import render_home
        render_home()


if __name__ == "__main__":
    main()


    # if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    #     if prompt.strip():
    #         chatbot = st.session_state.chatbot_service
    
    #         # GPTë¥¼ í†µí•´ ê°ì • ë¶„ì„
    #         dominant_emotion = get_emotion_from_gpt(prompt)
    
    #         # GPTë¡œë¶€í„° ì‘ë‹µ ìƒì„±
    #         response = chatbot.get_response(prompt)
    
    #         # í˜„ì¬ ì‹œê°„ ê¸°ë¡
    #         current_time = datetime.now().strftime('%p %I:%M')
    
    #         # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    #         st.session_state.messages.append({
    #             "role": "user",
    #             "content": prompt,
    #             "emotion": dominant_emotion,
    #             "timestamp": current_time
    #         })
    
    #         # GPT ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
    #         st.session_state.messages.append({
    #             "role": "assistant",
    #             "content": response,
    #             "timestamp": current_time
    #         })
    
    #         # í†µê³„ ì—…ë°ì´íŠ¸
    #         update_conversation_stats(dominant_emotion)
    
    #         # í™”ë©´ ê°±ì‹ 
    #         st.rerun()


    # # í…ìŠ¤íŠ¸ ì…ë ¥ì°½
    # if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    #     if prompt.strip():
    #         chatbot = st.session_state.chatbot_service
    #         emotions = chatbot.analyze_emotion(prompt)
    #         dominant_emotion = max(emotions.items(), key=lambda x: x[1])[0]
    #         response = chatbot.get_response(prompt)

    #         current_time = datetime.now().strftime('%p %I:%M')
    #         st.session_state.messages.append({
    #             "role": "user",
    #             "content": prompt,
    #             "emotion": dominant_emotion,
    #             "timestamp": current_time
    #         })
    #         st.session_state.messages.append({
    #             "role": "assistant",
    #             "content": response,
    #             "timestamp": current_time
    #         })

    #         # í†µê³„ ì—…ë°ì´íŠ¸
    #         update_conversation_stats(dominant_emotion)

    #         st.rerun()
