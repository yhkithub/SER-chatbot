from datetime import datetime
import streamlit as st
import torch
import torchaudio
import os
from transformers import AutoModelForAudioClassification, AutoProcessor
import torchaudio.transforms as T
from src.core.services.chatbot_service import ChatbotService
from src.app.config import OpenAIConfig

# ìŒì„± ê°ì • ì¸ì‹ ëª¨ë¸ ì„¤ì •
model_name = "forwarder1121/ast-finetuned-model"
processor = AutoProcessor.from_pretrained(model_name)
model = AutoModelForAudioClassification.from_pretrained(model_name)

# ê°ì • ë§¤í•‘
EMOTION_MAPPING = {
    0: "Anger",
    1: "Disgust", 
    2: "Fear",
    3: "Happy",
    4: "Neutral",
    5: "Sad"
}

def process_audio(waveform, target_sample_rate=16000, target_length=16000):
    """Process audio to correct format."""
    try:
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        
        if waveform.shape[1] > 0:
            current_sample_rate = target_sample_rate
            if current_sample_rate != target_sample_rate:
                resampler = T.Resample(orig_freq=current_sample_rate, new_freq=target_sample_rate)
                waveform = resampler(waveform)

        if waveform.shape[1] < target_length:
            padding_length = target_length - waveform.shape[1]
            waveform = torch.nn.functional.pad(waveform, (0, padding_length))
        else:
            start = (waveform.shape[1] - target_length) // 2
            waveform = waveform[:, start:start + target_length]
        
        return waveform
    except Exception as e:
        st.error(f"Error in audio processing: {str(e)}")
        return None

def predict_audio_emotion(audio_path):
    """Predict emotion from audio file."""
    try:
        waveform, sample_rate = torchaudio.load(audio_path)
        processed_waveform = process_audio(waveform, target_sample_rate=16000)
        
        if processed_waveform is None:
            return None
        
        inputs = processor(processed_waveform.squeeze(), sampling_rate=16000, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        predicted_class_idx = outputs.logits.argmax(-1).item()
        
        if predicted_class_idx in EMOTION_MAPPING:
            return EMOTION_MAPPING[predicted_class_idx]
        return None
            
    except Exception as e:
        st.error(f"Error in emotion prediction: {str(e)}")
        return None

def add_message(role, content, emotion=None):
    """Add a new message to the chat."""
    current_time = datetime.now().strftime('%p %I:%M')
    st.session_state.messages.append({
        "role": role,
        "content": content,
        "emotion": emotion,
        "timestamp": current_time
    })

def main():
    st.set_page_config(
        page_title="ê°ì •ì¸ì‹ ì±—ë´‡",
        page_icon="ðŸ¤—",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'initialized' not in st.session_state:
        st.session_state.initialized = True
        st.session_state.chatbot_service = ChatbotService(OpenAIConfig())
        st.session_state.messages = [{
            'role': 'assistant',
            'content': "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë– ì…¨ë‚˜ìš”? ê¸°ë¶„ì´ë‚˜ ê°ì •ì„ ìžìœ ë¡­ê²Œ ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”. í…ìŠ¤íŠ¸ë¡œ ìž…ë ¥í•˜ê±°ë‚˜ ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”. ðŸ˜Š",
            'timestamp': datetime.now().strftime('%p %I:%M')
        }]
        st.session_state.audio_processing = False  # ìŒì„± ì²˜ë¦¬ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.audio_message = None  # ìŒì„± ë©”ì‹œì§€ ì´ˆê¸°í™”

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.title("ê°ì •ì¸ì‹ ì±—ë´‡ ðŸ ")
        st.markdown("### ì‚¬ìš© ë°©ë²•")
        st.markdown("""
        1. ì±„íŒ…ì°½ì— í˜„ìž¬ ê¸°ë¶„ì´ë‚˜ ìƒí™©ì„ ìž…ë ¥í•˜ì„¸ìš”
        2. ë˜ëŠ” ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ê°ì •ì„ ë¶„ì„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤
        3. ì±—ë´‡ì´ ê°ì •ì„ ë¶„ì„í•˜ê³  ê³µê°ì ì¸ ëŒ€í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤
        4. í•„ìš”í•œ ê²½ìš° ì ì ˆí•œ ì¡°ì–¸ì´ë‚˜ ìœ„ë¡œë¥¼ ë°›ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤
        """)

        # ìŒì„± íŒŒì¼ ì—…ë¡œë”
        uploaded_audio = st.file_uploader("ìŒì„± íŒŒì¼ ì—…ë¡œë“œ", type=["wav", "mp3", "ogg"])
        if uploaded_audio is not None and not st.session_state.audio_processing:
            try:
                # ìŒì„± ì²˜ë¦¬ ì‹œìž‘ í”Œëž˜ê·¸
                st.session_state.audio_processing = True

                # ìž„ì‹œ íŒŒì¼ë¡œ ì €ìž¥
                with open("temp_audio.wav", "wb") as f:
                    f.write(uploaded_audio.getbuffer())

                # ìŒì„± ë¶„ì„
                with st.spinner('ìŒì„± ë¶„ì„ ì¤‘...'):
                    audio_emotion = predict_audio_emotion("temp_audio.wav")

                if audio_emotion:
                    # ìŒì„± ë©”ì‹œì§€ ì €ìž¥
                    st.session_state.audio_message = [
                        {
                            "role": "user",
                            "content": "[ìŒì„± íŒŒì¼ì´ ì—…ë¡œë“œë¨]",
                            "emotion": audio_emotion,
                            "timestamp": datetime.now().strftime('%p %I:%M')
                        },
                        {
                            "role": "assistant",
                            "content": f"ìŒì„±ì—ì„œ ê°ì§€ëœ ê°ì •ì€ {audio_emotion}ìž…ë‹ˆë‹¤. ë” ìžì„¸ížˆ ì´ì•¼ê¸°í•´ì£¼ì‹œê² ì–´ìš”?",
                            "timestamp": datetime.now().strftime('%p %I:%M')
                        }
                    ]
            except Exception as e:
                st.error(f"ìŒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            finally:
                # ìŒì„± ì²˜ë¦¬ ì™„ë£Œ í›„ íŒŒì¼ ì‚­ì œ
                if os.path.exists("temp_audio.wav"):
                    os.remove("temp_audio.wav")

    # ë©”ì¸ ì±„íŒ… ì˜ì—­
    st.title("ì±„íŒ…")

    # ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
    for message in st.session_state.get('messages', []):
        with st.chat_message(message["role"]):
            st.write(message["content"])
            if "emotion" in message:
                st.caption(f"ê°ì •: {message['emotion']}")
            st.caption(f"ì‹œê°„: {message['timestamp']}")

    # ìƒˆë¡œ ì—…ë¡œë“œëœ ìŒì„± ë©”ì‹œì§€ ì¶œë ¥
    if st.session_state.audio_message:
        for audio_msg in st.session_state.audio_message:
            with st.chat_message(audio_msg["role"]):
                st.write(audio_msg["content"])
                if "emotion" in audio_msg:
                    st.caption(f"ê°ì •: {audio_msg['emotion']}")
                st.caption(f"ì‹œê°„: {audio_msg['timestamp']}")

    # í…ìŠ¤íŠ¸ ìž…ë ¥ì°½
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ìž…ë ¥í•˜ì„¸ìš”..."):
        if prompt.strip():
            # í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ë¶„ì„ ë° ì‘ë‹µ
            chatbot = st.session_state.chatbot_service
            emotions = chatbot.analyze_emotion(prompt)
            dominant_emotion = max(emotions.items(), key=lambda x: x[1])[0]
            response = chatbot.get_response(prompt)

            current_time = datetime.now().strftime('%p %I:%M')

            # ë©”ì‹œì§€ ì €ìž¥
            st.session_state.messages.append({
                "role": "user",
                "content": prompt,
                "emotion": dominant_emotion,
                "timestamp": current_time
            })
            st.session_state.messages.append({
                "role": "assistant",
                "content": response,
                "timestamp": current_time
            })

            # ìŒì„± ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.audio_processing = False
            st.session_state.audio_message = None

            # ìƒˆ ë©”ì‹œì§€ ë Œë”ë§
            st.rerun()

if __name__ == "__main__":
    main()
